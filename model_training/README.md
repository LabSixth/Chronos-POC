
# Chronos & Prophet Training Microservice

This microservice is responsible for training time-series forecasting models using
[Amazon's Chronos](https://github.com/amazon-science/chronos-forecasting) and
[Facebook Prophet](https://facebook.github.io/prophet/). It is packaged as a container, pushed to Amazon Elastic Container Registry (ECR),
and executed using Amazon ECS Fargate.

---

## âš™ï¸ Prerequisite

1. AWS account and AWS CLI, and locally authenticated AWS for pushing Docker image to AWS ECR
2. Python virtual environment created, synced, and activated

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ config/                        # Configuration files
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ api/                       # API endpoints and server code
â”‚   â”‚   â””â”€â”€ main                   # Fast API calls
â”‚   â”œâ”€â”€ aws_utils/                 # AWS integration utilities
â”‚   â”‚   â””â”€â”€ aws_data_ingress.py    # Download data from AWS S3
|   |   â””â”€â”€ aws_data_outgress.py   # Export artifacts to AWS S3
â”‚   â”œâ”€â”€ data_processing/           # Data processing modules
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py       # Clean and prepare time series data
â”‚   â”‚   â”œâ”€â”€ data_loading.py        # Load data from various sources
â”‚   â”‚   â””â”€â”€ eda.py                 # Exploratory data analysis utilities
â”‚   â”œâ”€â”€ models/                    # Model implementation
â”‚   â”‚   â”œâ”€â”€ chronos_model.py       # Amazon Chronos model implementation
â”‚   â”‚   â””â”€â”€ prophet_model.py       # Facebook Prophet model implementation
â”‚   â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”‚   â””â”€â”€ model_evaluation.py    # Model evaluation functions
â”œâ”€â”€ test/                          # Test suite
â”‚   â””â”€â”€ test_data_processing.py    # Tests for data processing module
â”‚   â””â”€â”€ test_evaluation.py         # Tests for model evaluation module
â”‚   â””â”€â”€ test_models.py             # Tests for modeling module
â”œâ”€â”€ .dockerignore                  # Files to exclude from Docker context
â”œâ”€â”€ .python-version                # Python version specification
â”œâ”€â”€ Dockerfile                     # Container definition
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ pipeline.py                    # Main pipeline entry point
â”œâ”€â”€ pyproject.toml                 # Project metadata and dependencies
â””â”€â”€ requirements.txt               # Direct dependencies list
```

---

## ğŸš€ Key Functionality

1. `pipeline.py`

- Acts as the entrypoint for model training.
- Accepts CLI or environment variables pointing to a YAML config file.
- Loads configuration and routes to either Chronos or Prophet trainer.
- Uploads all model artifacts and metrics to a specified S3 bucket.

2. `src/models/chronos_model.py`

- Loads time-series data.
- Configures Chronos model based on YAML input.
- Trains Chronos model and logs metrics (e.g. MAE, RMSE).
- Saves model to disk.

3. `src/models/prophet_trainer.py`

- Similar to Chronos trainer but using Facebook Prophet.
- Handles Prophet model setup, forecasting, and metric evaluation.
- Outputs include forecast visualizations and metrics YAML.

3. `src/utils/model_evaluation.py`

- Computes accuracy metrics like MAE, RMSE, MAPE for model evaluation.

### Running Pipeline Locally

To run the pipeline locally, run the command below in command line.

```shell
python3 pipeline.py --config config/default-config.yaml
```

---

## ğŸ³ Dockerization

### Dockerfile

- Based on Python 3.11 Slim base image (compatible with ECS Fargate).
- Installs dependencies using `uv` and `pyproject.toml`.

To build and push:

```bash
docker build -t chronos-application .
aws ecr get-login-password | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<region>.amazonaws.com
docker tag chronos-application:latest <your-ecr-uri>
docker push <your-ecr-uri>
```

---

## â˜ï¸ ECS Fargate Deployment

- This image is deployed via ECS Fargate.
- It pulls the image from ECR, loads config from S3, trains model, and exports results.
